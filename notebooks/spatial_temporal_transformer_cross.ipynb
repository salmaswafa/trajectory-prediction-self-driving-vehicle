{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = 'spatial_temporal_transformer_cross.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "train_file = np.load('../cse-251-b-2025/train.npz')\n",
    "\n",
    "train_data = train_file['data']\n",
    "# train_data = train_data[::2]\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('../cse-251-b-2025/test_input.npz')\n",
    "\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDatasetTrain(Dataset):\n",
    "    def __init__(self, data, scale=10.0, augment=True):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Training data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        augment: Whether to apply data augmentation (only for training)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]\n",
    "        # Getting 50 historical timestamps and 60 future timestamps\n",
    "        hist = scene[:, :50, :].copy()    # (agents=50, time_seq=50, 6)\n",
    "        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (50, 60, 2)\n",
    "        #add the feature of the scene number for each sample\n",
    "       \n",
    " \n",
    "        # Data augmentation(only for training)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.5:\n",
    "                theta = np.random.uniform(-np.pi, np.pi)\n",
    "                R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
    "                # Rotate the historical trajectory and future trajectory\n",
    "                hist[..., :2] = hist[..., :2] @ R\n",
    "                hist[..., 2:4] = hist[..., 2:4] @ R\n",
    "                future = future @ R\n",
    "                # future[..., 2:4] = future[..., 2:4] @ R\n",
    "            if np.random.rand() < 0.5:\n",
    "                hist[..., 0] *= -1\n",
    "                hist[..., 2] *= -1\n",
    "                future[:, 0] *= -1\n",
    "                # future[:, 2] *= -1\n",
    "\n",
    "        # Use the last timeframe of the historical trajectory as the origin\n",
    "        origin = hist[0, 49, :2].copy()  # (2,)\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        # future[..., :2] = future[..., :2] - origin\n",
    "        future = future - origin\n",
    "\n",
    "        # Normalize the historical trajectory and future trajectory\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "        future = future / self.scale\n",
    "        # hist[..., :2] = hist[..., :2] / self.scale\n",
    "        # future[..., :2] = future[..., :2] / self.scale\n",
    "\n",
    "        \n",
    "        # print(\"hist's shape\", hist.shape)\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            y=future.type(torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0), # (1,2)\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32), # scalar e.g. 7.0\n",
    "        )\n",
    "\n",
    "        return data_item\n",
    "    \n",
    "\n",
    "class TrajectoryDatasetTest(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Testing data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Testing data only contains historical trajectory\n",
    "        scene = self.data[idx]  # (50, 50, 6)\n",
    "        hist = scene.copy()\n",
    "        # hist = hist[...,]\n",
    "        \n",
    "        origin = hist[0, 49, :2].copy()\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "        hist[..., :2] = hist[..., :2] / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "        return data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(251)\n",
    "np.random.seed(42)\n",
    "\n",
    "scale = 7.0 #why not 10\n",
    "\n",
    "N = len(train_data)\n",
    "val_size = int(0.1 * N)\n",
    "train_size = N - val_size\n",
    "\n",
    "train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "\n",
    "# Set device for training speedup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, agent_type_name, max_len=50):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.lstm = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        self.agent_type_embedding = nn.Parameter(torch.randn(1, 1, d_model))  # Learnable\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, max_len, d_model))  # Learnable\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, N, T, F = x.shape\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.agent_type_embedding + self.positional_encoding[:, :T]\n",
    "        x = x.view(B * N, T, -1)\n",
    "        output, _ = self.lstm(x)\n",
    "        output = output[:, -1, :]\n",
    "        # print('enc done')\n",
    "        return output.view(B, N, -1), mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleTrajectoryDecoder(nn.Module):\n",
    "    def __init__(self, d_model=128, output_dim=2, n_heads=4, T_pred=60):\n",
    "        super().__init__()\n",
    "        self.T_pred = T_pred\n",
    "        self.spatial_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
    "        self.temporal_decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)\n",
    "        self.temporal_decoder = nn.TransformerDecoder(self.temporal_decoder_layer, num_layers=1)\n",
    "        self.output_proj = nn.Linear(d_model, output_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, T_pred, d_model))\n",
    "\n",
    "    def forward(self, H_v_all, H_v0_init):\n",
    "        B, _, D = H_v_all.shape\n",
    "        tgt = H_v0_init  # [B, 1, 1, D]\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.T_pred):\n",
    "            tgt_pe = tgt + self.positional_encoding[:, :t+1].unsqueeze(1)  # [B, 1, t+1, D]\n",
    "            tgt_step = tgt_pe.view(B, t + 1, D)\n",
    "            memory = H_v_all  # [B, N_v, D]\n",
    "            # print(memory.shape, tgt_step[:, -1:, :].shape)\n",
    "\n",
    "            # Use memory of all agents for spatial context\n",
    "            attn_out, _ = self.spatial_attn(tgt_step[:, -1:, :], memory, memory)\n",
    "            context = attn_out  # [B, 1, D]\n",
    "            # print(\"context shape\", context.shape)\n",
    "\n",
    "            # Run temporal decoder\n",
    "            decoded = self.temporal_decoder(tgt_step, memory)  # [B, t+1, D]\n",
    "            next_token = decoded[:, -1:, :] + context  # Add spatial interaction\n",
    "\n",
    "            tgt = torch.cat([tgt, next_token.unsqueeze(1)], dim=2)  # [B, 1, t+2, D]\n",
    "            outputs.append(next_token.unsqueeze(1))  # [B, 1, 1, D]\n",
    "\n",
    "        out_seq = torch.cat(outputs, dim=2)  # [B, 1, T_pred, D]\n",
    "        coords = self.output_proj(out_seq)  # [B, 1, T_pred, output_dim]\n",
    "        return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleTrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=6, d_model=128, output_dim=2, n_heads=4, T_pred=60):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.output_dim = output_dim\n",
    "        self.vehicle_encoder = AgentEncoder(input_dim, d_model, \"vehicle\")\n",
    "        self.ped_encoder = AgentEncoder(input_dim, d_model, \"pedestrian\")\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
    "        self.decoder = VehicleTrajectoryDecoder(d_model, output_dim, n_heads, T_pred)\n",
    "\n",
    "    def forward(self,data):\n",
    "        # Encode vehicles and pedestrians\n",
    "        x = data.x[..., :self.input_dim] \n",
    "        x = x.view(-1, 50, 50, self.input_dim)   # [B, N, T, F]\n",
    "        B = x.shape[0]  # Batch size\n",
    "        #get agents where 6th feature is 0\n",
    "        #repeat p_m and v_m in the 3rd dimension\n",
    "\n",
    "        v_m  = (x[:,:,:,5]==0).any(axis=(0,2))\n",
    "        p_m  = (x[:,:,:,5]==1).any(axis=(0,2))\n",
    "        v_m = v_m.unsqueeze(0).unsqueeze(2).repeat(16,1,50)\n",
    "        p_m = p_m.unsqueeze(0).unsqueeze(2).repeat(16,1,50)\n",
    "        batch_x_v = x[v_m]  # [B, N_v, T, F]\n",
    "        batch_x_v.view(B,-1, 50, 6).shape\n",
    "        batch_x_p = x[p_m]  # [B, N_p, T, F]\n",
    "        vehicles = batch_x_v.view(B, -1, 50, 6)  # [B, N_v, T, F]\n",
    "        peds = batch_x_p.view(B, -1, 50, 6)  # [B, N_p, T, F]\n",
    "        \n",
    "        N_v = vehicles.shape[1]\n",
    "        N_p = peds.shape[1]\n",
    "        \n",
    "        v_mask = torch.ones(B, N_v).bool().to(vehicles.device)  # Mask for vehicles\n",
    "        p_mask = torch.ones(B, N_p).bool().to(peds.device)  # Mask for pedestrians\n",
    "\n",
    "\n",
    "        H_v, _ = self.vehicle_encoder(vehicles, v_mask)  # [B, N_v, D]\n",
    "        H_p, _ = self.ped_encoder(peds, p_mask)          # [B, N_p, D]\n",
    "\n",
    "        B, N_v, D = H_v.shape\n",
    "        N_p = H_p.shape[1]\n",
    "\n",
    "        # Cross-attention: vehicles attend to pedestrians\n",
    "        H_v_flat = H_v.view(B * N_v, 1, D)\n",
    "        H_p_exp = H_p.unsqueeze(1).expand(B, N_v, N_p, D).contiguous().view(B * N_v, N_p, D)\n",
    "        p_mask_exp = ~p_mask.unsqueeze(1).expand(B, N_v, N_p).contiguous().view(B * N_v, N_p)\n",
    "\n",
    "        attn_out, _ = self.cross_attn(H_v_flat, H_p_exp, H_p_exp, key_padding_mask=p_mask_exp)\n",
    "        H_v_cross = attn_out.view(B, N_v, D)\n",
    "        H_v_enhanced = H_v + H_v_cross  # [B, N_v, D]\n",
    "\n",
    "        # Initial decoder input for vehicle[0]\n",
    "        H_v0_init = H_v_enhanced[:, 0:1, :].unsqueeze(2)  # [B, 1, 1, D]\n",
    "        pred_traj = self.decoder(H_v_enhanced, H_v0_init)  # [B, 1, T_pred, output_dim]\n",
    "\n",
    "        return pred_traj.squeeze(1)  # [B, T_pred, output_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_model(model, train_dataloader, val_dataloader, \n",
    "                         device, criterion=nn.MSELoss(), \n",
    "                         lr=0.001, epochs=100, patience=15):\n",
    "    \"\"\"\n",
    "    Improved training function with better debugging and early stopping\n",
    "    \"\"\"\n",
    "    # Initialize optimizer with smaller learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Exponential decay scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    \n",
    "    early_stopping_patience = patience\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Save initial state for comparison\n",
    "    initial_state_dict = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    for epoch in tqdm.tqdm(range(epochs), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        # ---- Training ----\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_train_batches = 0\n",
    "        forcing_ratio = max(0.0, 1.0 - epoch / 50)\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "            \n",
    "            # Check for NaN predictions\n",
    "            if torch.isnan(pred).any():\n",
    "                print(f\"WARNING: NaN detected in predictions during training\")\n",
    "                continue\n",
    "                \n",
    "            loss = criterion(pred, y)\n",
    "            \n",
    "            # Check if loss is valid\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"WARNING: Invalid loss value: {loss.item()}\")\n",
    "                continue\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # More conservative gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        # Skip epoch if no valid batches\n",
    "        if num_train_batches == 0:\n",
    "            print(\"WARNING: No valid training batches in this epoch\")\n",
    "            continue\n",
    "            \n",
    "        train_loss /= num_train_batches\n",
    "        \n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "        val_mse = 0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        # Sample predictions for debugging\n",
    "        sample_input = None\n",
    "        sample_pred = None\n",
    "        sample_target = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_dataloader):\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "                \n",
    "                # Store sample for debugging\n",
    "                if batch_idx == 0 and sample_input is None:\n",
    "                    sample_input = batch.x[0].cpu().numpy()\n",
    "                    sample_pred = pred[0].cpu().numpy()\n",
    "                    sample_target = y[0].cpu().numpy()\n",
    "                \n",
    "                # Skip invalid predictions\n",
    "                if torch.isnan(pred).any():\n",
    "                    print(f\"WARNING: NaN detected in predictions during validation\")\n",
    "                    continue\n",
    "                    \n",
    "                batch_loss = criterion(pred, y).item()\n",
    "                val_loss += batch_loss\n",
    "                \n",
    "                # Unnormalize for real-world metrics\n",
    "                # batch.scale turns scale from 7.0 or (1,) shape i.e. scalar to (B,) shape\n",
    "                # batch.origin turns origin from (1,2) shape to (B,2)\n",
    "                \n",
    "                # then .view(-1, 1, 1) turns scale from (B,) to (B, 1, 1)\n",
    "                # then .unsqueeze(1) turns origin from (B, 2) to (B, 1, 2)\n",
    "                # because pred and y have shapes (B, 60, 2) so these transformations make them compatible for the calculation\n",
    "                \n",
    "                pred_unnorm = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                y_unnorm = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                \n",
    "                val_mae += nn.L1Loss()(pred_unnorm, y_unnorm).item()\n",
    "                val_mse += nn.MSELoss()(pred_unnorm, y_unnorm).item()\n",
    "                \n",
    "                num_val_batches += 1\n",
    "        \n",
    "        # Skip epoch if no valid validation batches\n",
    "        if num_val_batches == 0:\n",
    "            print(\"WARNING: No valid validation batches in this epoch\")\n",
    "            continue\n",
    "            \n",
    "        val_loss /= num_val_batches\n",
    "        val_mae /= num_val_batches\n",
    "        val_mse /= num_val_batches\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print with more details\n",
    "        tqdm.tqdm.write(\n",
    "            f\"Epoch {epoch:03d} | LR {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "            f\"Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f} | \"\n",
    "            f\"Val MAE {val_mae:.4f} | Val MSE {val_mse:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Debug output - first 3 predictions vs targets\n",
    "        if epoch % 5 == 0:\n",
    "            tqdm.tqdm.write(f\"Sample pred first 3 steps: {sample_pred[:3]}\")\n",
    "            tqdm.tqdm.write(f\"Sample target first 3 steps: {sample_target[:3]}\")\n",
    "            \n",
    "            # Check if model weights are changing\n",
    "            if epoch > 0:\n",
    "                weight_change = False\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        initial_param = initial_state_dict[name]\n",
    "                        if not torch.allclose(param, initial_param, rtol=1e-4):\n",
    "                            weight_change = True\n",
    "                            break\n",
    "                if not weight_change:\n",
    "                    tqdm.tqdm.write(\"WARNING: Model weights barely changing!\")\n",
    "        \n",
    "        # Relaxed improvement criterion - consider any improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            tqdm.tqdm.write(f\"Validation improved: {best_val_loss:.6f} -> {val_loss:.6f}\")\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), best_model)\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= early_stopping_patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs without improvement\")\n",
    "                break\n",
    "    \n",
    "    # Load best model before returning\n",
    "    model.load_state_dict(torch.load(best_model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def train_and_evaluate_model():\n",
    "    # Create model\n",
    "    model = VehicleTrajectoryPredictor()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Train with improved function\n",
    "    train_improved_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        device=device,\n",
    "        # lr = 0.007 => 8.946\n",
    "        lr=0.007,  # Lower learning rate\n",
    "        patience=20,  # More patience\n",
    "        epochs=100\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    test_mse = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "            \n",
    "            # Unnormalize\n",
    "            pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            y = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            \n",
    "            test_mse += nn.MSELoss()(pred, y).item()\n",
    "    \n",
    "    test_mse /= len(val_dataloader)\n",
    "    print(f\"Val MSE: {test_mse:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/100 [00:00<?, ?epoch/s]/var/folders/0r/zhth93bs1ygg_ln8t_nhb4f80000gn/T/ipykernel_98682/3067390390.py:44: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  future = future - origin\n",
      "/var/folders/0r/zhth93bs1ygg_ln8t_nhb4f80000gn/T/ipykernel_98682/3067390390.py:32: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  future = future @ R\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
